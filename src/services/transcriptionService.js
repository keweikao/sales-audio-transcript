const fs = require('fs');
const path = require('path');
const ffmpeg = require('fluent-ffmpeg');
const tmp = require('tmp');
const winston = require('winston');
const { spawn } = require('child_process');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [new winston.transports.Console()]
});

// Configuration for iPhone recordings and chunking (adapted from remote)
const IPHONE_OPTIMIZED_CONFIG = {
  // Using 'base' model for OpenAI whisper (can be changed to 'large-v3' if resources allow)
  modelName: 'base',
  // Chunking strategy for iPhone recordings - å¢åŠ åˆ° 30 åˆ†é˜é¿å…éå¤šåˆ†å¡Š
  chunkDuration: 30 * 60, // 30 minutes per chunk, to avoid too many parallel processes
  // Audio preprocessing parameters (already in preprocessiPhoneAudio)
  preprocessing: {
    bitrate: 64,
    sampleRate: 16000,
    channels: 1,
    filters: [
      'highpass=f=80',
      'lowpass=f=8000'
    ]
  },
  // OpenAI whisper specific options
  whisperOptions: {
    language: 'zh'
  }
};

/**
 * Calls the Python script to transcribe audio using OpenAI whisper.
 * @param {string} audioPath The path to the audio file.
 * @returns {Promise<string>} A promise that resolves with the transcribed text.
 */
// å…¨åŸŸè®Šæ•¸è¿½è¹¤ç•¶å‰é€²ç¨‹ï¼Œç¢ºä¿åŒæ™‚åªæœ‰ä¸€å€‹ Whisper é€²ç¨‹é‹è¡Œ
function transcribeWithOpenAIWhisper(audioPath) {
  return new Promise((resolve, reject) => {
    const pythonScriptPath = path.join(__dirname, 'whisper_transcribe.py');
    logger.info(`ğŸš€ å•Ÿå‹• Python Whisper é€²ç¨‹: ${path.basename(audioPath)}`);
    
    const pythonProcess = spawn('python3', [pythonScriptPath, audioPath, '--output-json'], {
      timeout: 29 * 60 * 1000 // 29 minutes timeout for the Python process
    });

    let transcript = '';
    let errorMessage = '';
    let scriptExited = false;
    let stdoutClosed = false;
    let stderrClosed = false;

    const tryResolve = () => {
      // Ensure we only resolve/reject once, and only after the process has exited
      // and all output streams are closed.
      if (scriptExited && stdoutClosed && stderrClosed) {
        if (pythonProcess.exitCode === 0) {
          logger.info('âœ… Python script finished successfully.');
          try {
            const result = JSON.parse(transcript.trim());
            resolve(result);
          } catch (parseError) {
            logger.warn('Could not parse Python script output as JSON, falling back to plain text.');
            resolve(transcript.trim());
          }
        } else {
          logger.error(`âŒ Python script exited with code ${pythonProcess.exitCode}`);
          reject(new Error(`Transcription failed with exit code ${pythonProcess.exitCode}. Error: ${errorMessage}`));
        }
      }
    };

    pythonProcess.stdout.on('data', (data) => {
      transcript += data.toString();
    });
    pythonProcess.stdout.on('close', () => {
      stdoutClosed = true;
      tryResolve();
    });

    pythonProcess.stderr.on('data', (data) => {
      const stderrLine = data.toString();
      logger.error(`[Python Script]: ${stderrLine}`);
      errorMessage += stderrLine;
    });
    pythonProcess.stderr.on('close', () => {
      stderrClosed = true;
      tryResolve();
    });

    pythonProcess.on('exit', (code) => {
      scriptExited = true;
      pythonProcess.exitCode = code; // Store exit code
      logger.info(`ğŸ”„ Python Whisper é€²ç¨‹çµæŸ`);
      tryResolve();
    });

    pythonProcess.on('error', (err) => {
      logger.error('âŒ Failed to start Python script.', err);
      reject(err);
    });
    
    pythonProcess.on('timeout', () => {
      logger.error('â° Python script timed out.');
      pythonProcess.kill();
    });
  });
}

/**
 * Gets detailed information about an audio file.
 */
async function getAudioInfo(filePath) {
  return new Promise((resolve, reject) => {
    ffmpeg.ffprobe(filePath, (err, metadata) => {
      if (err) {
        reject(err);
        return;
      }
      
      const format = metadata.format;
      const audioStream = metadata.streams.find(s => s.codec_type === 'audio');
      
      resolve({
        duration: format.duration,
        sizeMB: format.size / (1024 * 1024),
        bitrate: format.bit_rate,
        format: format.format_name,
        codec: audioStream?.codec_name,
        sampleRate: audioStream?.sample_rate,
        channels: audioStream?.channels,
        isFromiPhone: detectiPhoneRecording(metadata)
      });
    });
  });
}

/**
 * Detects if the recording is from an iPhone based on metadata.
 */
function detectiPhoneRecording(metadata) {
  const format = metadata.format;
  const isiPhoneFormat = format.format_name?.includes('mov') || 
                        format.format_name?.includes('mp4') ||
                        format.format_name?.includes('m4a');
  const audioStream = metadata.streams.find(s => s.codec_type === 'audio');
  const isiPhoneCodec = audioStream?.codec_name === 'aac';
  return isiPhoneFormat && isiPhoneCodec;
}

/**
 * Pre-processes audio files, especially for iPhone recordings.
 */
async function preprocessiPhoneAudio(inputPath, outputPath, audioInfo) {
  return new Promise((resolve, reject) => {
    logger.info(`Starting audio preprocessing for: ${inputPath}`);
    
    const ffmpegCommand = ffmpeg(inputPath)
      .audioCodec('libmp3lame')
      .audioBitrate('64k')
      .audioFrequency(16000)
      .audioChannels(1)
      .audioFilters('highpass=f=80', 'lowpass=f=8000')
      .output(outputPath);
    
    ffmpegCommand
      .on('start', (commandLine) => {
        logger.info(`FFmpeg command: ${commandLine}`);
      })
      .on('end', () => {
        logger.info(`Preprocessing finished: ${outputPath}`);
        resolve(outputPath);
      })
      .on('error', (err) => {
        logger.error(`Preprocessing failed: ${err.message}`);
        reject(err);
      })
      .run();
  });
}

/**
 * Extracts an audio chunk (from remote)
 */
async function extractChunk(inputPath, startTime, endTime) {
  const chunkPath = tmp.tmpNameSync({ postfix: `_chunk_${Date.now()}.wav` });
  
  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .seekInput(startTime)
      .duration(endTime - startTime)
      .audioCodec('pcm_s16le')
      .audioFrequency(16000)  // whisper-node requires 16kHz
      .audioChannels(1)
      .format('wav')
      .output(chunkPath)
      .on('end', () => resolve(chunkPath))
      .on('error', reject)
      .run();
  });
}

/**
 * Splits audio by time into chunks (from remote)
 */
async function splitByTime(inputPath, chunkDuration) {
  const audioInfo = await getAudioInfo(inputPath);
  const totalDuration = audioInfo.duration;
  const numChunks = Math.ceil(totalDuration / chunkDuration);
  
  const chunks = [];
  
  for (let i = 0; i < numChunks; i++) {
    const startTime = i * chunkDuration;
    const endTime = Math.min((i + 1) * chunkDuration, totalDuration);
    
    const chunkPath = await extractChunk(inputPath, startTime, endTime);
    chunks.push(chunkPath);
    
    logger.info(`å‰µå»ºæ™‚é–“ç‰‡æ®µ ${i + 1}/${numChunks}: ${(startTime/60).toFixed(1)}-${(endTime/60).toFixed(1)} åˆ†é˜`);
  }
  
  return chunks;
}

/**
 * Assesses the quality of the transcribed text.
 */
function assessTranscriptionQuality(transcript) {
  if (!transcript || transcript.length === 0) {
    return { score: 0, confidence: 0.0, details: 'Empty transcript' };
  }

  let score = 100;
  let confidence = 1.0;
  
  if (transcript.length < 10) {
    score -= 50;
    confidence -= 0.5;
  }
  
  const chineseRatio = (transcript.match(/[\u4e00-\u9fff]/g) || []).length / transcript.length;
  if (chineseRatio < 0.5) {
    score -= 20;
    confidence -= 0.2;
  }

  return {
    score: Math.max(0, Math.min(100, score)),
    confidence: Math.max(0, Math.min(1, confidence)),
    chineseRatio
  };
}

/**
 * Main transcription function that orchestrates the process.
 */
async function transcribeAudio(inputPath) {
  const tempDir = tmp.dirSync({ unsafeCleanup: true });
  try {
    logger.info(`Starting transcription process for: ${inputPath}`);
    
    // 1. Get audio info
    const audioInfo = await getAudioInfo(inputPath);
    logger.info(`Audio info retrieved: ${audioInfo.duration}s, ${audioInfo.sizeMB.toFixed(2)}MB`);

    let fullTranscript = '';

    // Check if chunking is needed for very long audio
    if (audioInfo.duration > IPHONE_OPTIMIZED_CONFIG.chunkDuration) {
      logger.info(`Audio duration (${audioInfo.duration}s) exceeds chunk duration (${IPHONE_OPTIMIZED_CONFIG.chunkDuration}s). Splitting into chunks.`);
      const chunks = await splitByTime(inputPath, IPHONE_OPTIMIZED_CONFIG.chunkDuration);

      // ğŸ”„ åš´æ ¼åºåˆ—è™•ç†æ¯å€‹ chunkï¼Œä¸€æ¬¡åªè™•ç†ä¸€å€‹ï¼Œé¿å…è³‡æºéè¼‰
      logger.info(`ğŸ“Š é–‹å§‹åºåˆ—è™•ç† ${chunks.length} å€‹éŸ³æª”ç‰‡æ®µ`);
      
      for (let i = 0; i < chunks.length; i++) {
        const chunkPath = chunks[i];
        logger.info(`ğŸµ é–‹å§‹è™•ç†ç‰‡æ®µ ${i + 1}/${chunks.length}: ${path.basename(chunkPath)}`);
        logger.info(`ğŸ“‚ ç‰‡æ®µè·¯å¾‘: ${chunkPath}`);
        
        // æ­¥é©Ÿ 1: é è™•ç†éŸ³æª”
        const processedPath = path.join(tempDir.name, `processed_chunk_${i}.mp3`);
        logger.info(`ğŸ”§ é è™•ç†ç‰‡æ®µ ${i + 1}...`);
        await preprocessiPhoneAudio(chunkPath, processedPath, audioInfo);
        
        // æ­¥é©Ÿ 2: è½‰éŒ„è™•ç†ï¼ˆç¢ºä¿ä¸€å€‹å®Œæˆå¾Œæ‰é–‹å§‹ä¸‹ä¸€å€‹ï¼‰
        logger.info(`ğŸ¤– è½‰éŒ„ç‰‡æ®µ ${i + 1}ï¼Œç­‰å¾… Whisper è™•ç†å®Œæˆ...`);
        const startTime = Date.now();
        
        const result = await transcribeWithOpenAIWhisper(processedPath);
        
        const endTime = Date.now();
        const processingTime = Math.round((endTime - startTime) / 1000);
        
        const chunkTranscript = typeof result === 'string' ? result : result.text;
        fullTranscript += chunkTranscript + ' ';
        
        logger.info(`âœ… ç‰‡æ®µ ${i + 1} è½‰éŒ„å®Œæˆ - è€—æ™‚: ${processingTime}ç§’, æ–‡å­—é•·åº¦: ${chunkTranscript.length}`);
        
        // æ¸…ç†ç•¶å‰ç‰‡æ®µçš„è‡¨æ™‚æª”æ¡ˆï¼Œé‡‹æ”¾ç©ºé–“
        try {
          if (fs.existsSync(chunkPath)) fs.unlinkSync(chunkPath);
          if (fs.existsSync(processedPath)) fs.unlinkSync(processedPath);
          logger.info(`ğŸ—‘ï¸ å·²æ¸…ç†ç‰‡æ®µ ${i + 1} çš„è‡¨æ™‚æª”æ¡ˆ`);
        } catch (cleanupErr) {
          logger.warn(`âš ï¸ æ¸…ç†ç‰‡æ®µ ${i + 1} è‡¨æ™‚æª”æ¡ˆå¤±æ•—: ${cleanupErr.message}`);
        }
        
        // æ‰‹å‹•è§¸ç™¼åƒåœ¾å›æ”¶
        if (global.gc) {
          logger.info(`ğŸ§¹ æ‰‹å‹•è§¸ç™¼åƒåœ¾å›æ”¶...`);
          global.gc();
        }
        
        // åœ¨ç‰‡æ®µä¹‹é–“åŠ å…¥çŸ­æš«å»¶é²ï¼Œç¢ºä¿è³‡æºå®Œå…¨é‡‹æ”¾
        if (i < chunks.length - 1) {
          logger.info(`â¸ï¸ ç‰‡æ®µ ${i + 1} è™•ç†å®Œæˆï¼Œç­‰å¾… 2 ç§’å¾Œè™•ç†ä¸‹ä¸€ç‰‡æ®µ...`);
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
      
      logger.info(`ğŸ‰ æ‰€æœ‰ ${chunks.length} å€‹ç‰‡æ®µåºåˆ—è™•ç†å®Œæˆï¼Œç¸½æ–‡å­—é•·åº¦: ${fullTranscript.length}`);
    } else {
      // 2. Pre-process audio (single file)
      const processedPath = path.join(tempDir.name, 'processed.mp3');
      await preprocessiPhoneAudio(inputPath, processedPath, audioInfo);

      // 3. Transcribe using OpenAI whisper python script (single file)
      const result = await transcribeWithOpenAIWhisper(processedPath);
      fullTranscript = typeof result === 'string' ? result : result.text;
      logger.info(`Transcription received from Python script. Length: ${fullTranscript.length}`);
    }

    // 4. Assess quality
    const quality = assessTranscriptionQuality(fullTranscript);
    logger.info(`Transcription quality assessed: Score ${quality.score}, Confidence ${quality.confidence}`);

    return {
      transcript: fullTranscript,
      quality: quality,
      audioInfo: audioInfo
    };

  } catch (error) {
    logger.error(`Full transcription process failed: ${error.message}`);
    throw error;
  } finally {
    // Clean up the temporary directory
    try {
      if (fs.existsSync(tempDir.name)) {
        fs.rmSync(tempDir.name, { recursive: true, force: true });
        logger.info(`Temporary directory cleaned up: ${tempDir.name}`);
      }
    } catch (cleanupError) {
      logger.warn(`Failed to clean up temporary directory: ${cleanupError.message}`);
    }
  }
}

module.exports = {
  transcribeAudio,
  getAudioInfo,
  preprocessiPhoneAudio,
  assessTranscriptionQuality
};